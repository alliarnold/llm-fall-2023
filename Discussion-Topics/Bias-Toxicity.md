# Bias & Toxicity in LLM

### What is it?

**Bias** is the same in and outside of LLMs; it is a preference for or against a group of people, things, or characteristics. Where **toxicity** comes into play in LLMs is as an outcome of the bias in the model, be it from the code, training technique, or the training corpus. Broadly speaking, **Bias** is the cause of **toxicity** in LLMs. LLMs go from useful to dangerous when the biases in their base code and training corpus get activated through the ill-suited *application*.

### Why does it matter and why do we care

An LLM built and used on a closed network for specific training research may never create a toxic result or hurt the researchers interacting with it. However, that same LLM could have the potential to create toxic results and have damaging impacts if/when released into the world without guardrails, especially if it is given power over automated decisions. 

The bias and toxicity in LLMs matter when we ask how LLMs should be used, regulated, or accessed by the general population. In the United States corporations wield significant power. LLMs are being used for broad commercial purposes, while the latest releases and corporate applications for AI technology far outpace greater institutional planning for guardrails and protections. 

LLMs that employ toxic vernacular are already used to write articles, draft legal briefs, and outline marketing campaigns. And, moreover, many people erroneously associate computer-generated answers with empirical truth. While folks with a deeper understanding of statistics, code, and internet infrastructure understand that LLMs and other AI technologies are built on data as flawed as the world in which it was collected, that is less clear to those working outside of the technology and mathematics.

### What do we do about

There are many schools of thought and potential approaches to preventing damage caused by the application of LLMs. A few include:

* establish greater regulation [governmental, IRB, AI constitution that all other AI programs must go through before submitting outputs, etc]
* greater transparency [watermarks showing sources for all results, digital bibliographies, larger/more explicit text identifying the use of AI]
* change the training corpora [better-vetted materials, original materials, smaller corpora, larger corpora]
* fix the models [coding/training]
* micro-LLMs for tailored for smaller, specific applications

All the options have potential benefits, and I'm in favor of pushing for guardrails. We need as much effort, research, and advancement put into making LLMs and other AI models safe as folks are putting into making LLMs and AI for profitable
