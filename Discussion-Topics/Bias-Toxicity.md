# Bias & Toxicity in LLM

### What is it?

**Bias** in LLM is the same as outside of LLM, a preference for or against a group of people, things, or characteristics. Where **toxicity** comes to play in LLM is as an outcome of the bias in the model, be it from the code, training technique, or the training corpus. **Bias** is the cause of **toxicity** in LLMs. As fruit can go from nutritious to sickening when it goes rancid, LLMs go from useful to dangerous when the biases in their base code and corpus are manifested by the wrong *application*.

### Why does it matter and why do we care

A LLM built on a biased corpus, if only used on a closed network for specific training research, may never create a toxic result or have a negative impact on the researchers interacting with it. However, that LLM will always have the potential to create toxic results and have damaging impacts if/when released into the general world without guardrails, especially if it is given power over automated decisions that impact others. 

The bias and toxicity in LLMs matter when we ask how LLMs should be used, regulated, or accessed by the general population. At least in the United States, we live in a highly capitalistic society in which corporations wield significant power. LLMs are being used for broad commercial purposes, while institutional planning for guardrails and protections is far outpaced by the latest releases and corporate applications for AI technology. 

LLMs that employ toxic vernacular are used to write articles, draft legal briefs, and outline marketing campaigns. And, moreover, there is the unfortunate habit of many humans to see computer-generated outcomes as trustworthy, truthful, and empirically correct.

### What do we do about
