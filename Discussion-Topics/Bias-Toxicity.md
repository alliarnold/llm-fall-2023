# Bias & Toxicity in LLM

### What is it?

**Bias** is the same in and outside of LLMs; it is a preference for or against a group of people, things, or characteristics. Where **toxicity** comes into play in LLMs is as an outcome of the bias in the model, be it from the code, training technique, or the training corpus. **Bias** is the cause of **toxicity** in LLMs. As fruit can go from nutritious to sickening when it goes rancid, LLMs go from useful to dangerous when the biases in their base code and corpus get activated through the wrong *application*.

### Why does it matter and why do we care

A LLM built on a biased corpus, if only used on a closed network for specific training research, may never create a toxic result or have a negative impact on the researchers interacting with it. However, that LLM will always have the potential to create toxic results and have damaging impacts if/when released into the world without guardrails, especially if it is given power over automated decisions that impact life. 

The bias and toxicity in LLMs matter when we ask how LLMs should be used, regulated, or accessed by the general population. At least in the United States, we live in a highly capitalistic society in which corporations wield significant power. LLMs are being used for broad commercial purposes, while greater institutional planning for guardrails and protections is far outpaced by the latest releases and corporate applications for AI technology. 

LLMs that employ toxic vernacular are already used to write articles, draft legal briefs, and outline marketing campaigns. And, moreover, there is the unfortunate habit of many humans to see computer-generated outcomes as trustworthy, truthful, and empirically correct. While folks with a deeper understanding of statistics, code, and internet infrastructure understand that LLMs and other AI are built on data that is as flawed as the world in which it was collected, that is less clear to those working outside of the technology and mathematics.

### What do we do about

There are many schools of thought and potential approaches to preventing damage caused by the application of LLMs. A few include:

* establish greater regulation [governmental, IRB, AI constitution that all other AI programs must go through before submitting outputs, etc]
* greater transparency [watermarks showing sources for all results, digital bibliographies, larger/more explicit text identifying the use of AI]
* change the training corpora [better-vetted materials, original materials, smaller corpora, larger corpora]
* fix the models [coding/training]
* micro-LLMs for tailored for smaller, specific applications

All the options have potential benefits, and frankly, I'm in favor of *trying* to push for any and all guardrails. We need as much effort, research, and advancement put into making LLMs and other AI models safe and kind as folks are putting into making LLMs and AI for profitability. I personally think the first two have the best shot 
