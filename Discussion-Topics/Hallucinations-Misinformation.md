# Hallucinations & Misinformation in LLMs

## Misinformation

Scientific researchers looking into the impact of LLMs on **misinformation** are raising alarm bells with good reason. False LLM-produced texts are already visible in areas ranging from the wrong but mostly frustrating (such as fake product reviews) to the scary and highly consequential (like influencing elections). Misinformation, misleading reporting, and unscientific studies were an area of growing concern even before taking LLMs into account. 

Since the dawn of the internet and the start of social media, it has become clear that human culture needs to find new skills for assessing information, establishing trustworthy sources, and learning when, how, and what to share. LLMs are certainly adding greater urgency to the issue, but I believe the best solutions for addressing misformation will be found by social scientists, public advocates, and humanist regulators.

## Hallucinations
### The importance of distinction between hallucinations and disinformation for LLMs

Nonetheless, when discussing misinformation created by LLMs, it is important to understand the difference between a *hallucination* and *disinformation*. LLM outputs depend on a combination of their weights, training corpuses, and in RAG systems, research databases. The distinction between a *hallucination* and *disinformation* in inaccurate LLM outputs can point you to the model's architecture or its ground truth. Knowing where to look is a first step in repairing an issue and preventing further damage. A LLM that produces a high number of *hallucinations* can be fine-tuned to reduce the number of miscalculated responses or to respond to unknown prompts with outputs along the lines of, "no available answer." Conversely, *disinformation* comes up more often with LLMs that are given faulty training data, or are using poorly vetted databases within their RAG systems.

Understanding this distinction will likely become more important as more LLM generated misinformation use-cases make their way to the courtsystem. So far, the United States, social media companies haven't been held accountable for what is shared on them, with the companies distinguished as platforms not, the content. However, legal experts anticipate that there is greater potential for accountability when going after a social media system's technological design. New legal action coming from the state of New York is seeking to require social media companies to shift their algorithims and user interfaces for accounts by persons under 18. If successful, this could lay the groundwork for future checks-and-balances against businesses that use LLMs to create content. By this logic, published content with disformation could be blamed on the company that published the text, and hallucinations could be blamed on the company that created the LLM. We will see how this develops in the coming months and years, but I am sure that there will be legal challenges and precedents to follow soon enough.