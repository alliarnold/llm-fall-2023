# Privacy, Copyright & IP in LLM

## Privacy
### Zeroing in on LLMs


The wide-ranging applications of machine learning pose significant risks to privacy and our understanding of copyright and intellectual property (IP). However, when focusing on LLMs, privacy is the least concerning. There are growing threats to digital and physical privacy through unmitigated applications of AI and surveillance technology, but those are more specific to AI in imagery, audio, and location tracking.


Today, privacy risks with LLMs are more hypothetical. Through access to past emails, social media posts, or private text messages, LLMs can learn to reproduce someoneâ€™s written voice for impersonation or to associate someone (either accurately or inaccurately) with previously anonymous texts. However, that threat is unlikely to impact the average citizen.


## Copyright & IP


When thinking about privacy as the right to control the distribution of your information, it is an easy jump to consider your right to the profitability of your information and the ownership of your *personal* written labor.


Generally, **copyright** and **IP** in the United States are considered in terms of their legal definitions. While there are a few use cases, questions related to the legal rights of LLMs and those whose labor trained them are still evolving.


Opinions on what is allowed, what *should* be allowed, and what is *fair* vary wildly. From what I've witnessed, someone's level of power and view on corporations aligns closely with how much regulation they want for LLMs. Of those I've spoken to, wealthier people and those in positions of power tend to be more comfortable with LLMs scraping texts from sources without limitation. The people I spoke with who had lower trust for corporations, who had experienced being made professionally redundant, or who have felt a limited sense of control in the workforce, expressed more concern for the myriad ways that corporations can/could passively steal or make use of texts for LLM corpora.


Given the proliferation of LLMs, I think it is time to reassess what is fair use for research. We need to ask if the research in question is publically beneficial. Can something be for the public good and be unregulated? Does it matter if the ultimate application of the research is for corporate profit? Should an innately derivative tool be allowed to use training texts without the payment or approval of the authors? Should a private corporation whose primary goal is to create a profitable tool be allowed to cite research as a reason for not paying its sources? Especially if said tool disadvantages workers?


In the past, governments have demonstrated how to thoughtfully regulate the transition from one technology to another with protections for the original labor and the intellectual property reproduced, including paintings to prints to photography, web publication vs print, film to at-home video, streaming, etc. And yet, we seem to be crossing this machine learning threshold with a much looser concept of labor rights, copyright, and IP than we did in the past. Perhaps this is because of the speed of technological advancements, but I think it is more likely, or at least equally likely, a result of the far greater power that corporations hold today.


As things stand, laborers and creatives are put in the position of choosing between opting out of work, not sharing their work publically, or relinquishing their ownership (to some degree). More regulation is needed to protect the fruits of human labor, whether paid or voluntary. Regulation is especially needed to protect those early in their careers and those at greater economic risk.