# Hello, Eightbert!

## *how are you today?*
##**_Outlook good_**

### About

Magic Eightbert is a fine-tuned BERT model here to answer the questions on your mind today! Just kidding, Magic Eightbert doesn't have the answers, but that's why he's here - to help demonstrate the difference between probability-based responses and a time-machine-backed glimpse into the future.

AI and machine learning tools are the shiniest toys on the market today, promising everything from the end of humankind as we know it to a solution for all our problems. Thinkpeaces and government groups are dedicating a lot of time, reasonably so, to considering how much labor, decision-making, and trust should be given to LLMs and other machine learning tools, particularly when it comes to predicting an optimal future for humanity. 

Those of us who have had the privilege of studying statistics or the basics of machine learning have had the chance to learn something that many people haven't. Having seen how data can be converted into predictions, weights, and probabilities, we understand that LLMs are only as good as their architecture and training data, or "ground truth" as it is so affectionately misnamed. 

The truth is that humans have collectively agreed on very little when it comes to what is empirically true or just, particularly when it comes to issues with major moral and/or societal implications. The logical extension of accepting that is to also accept that LLM in existence has been trained only **true** with a capital "T" information. Whether or not you believe in hallucinations or see every output an LLM produces as equally hallucinogenic, 

I will fine-tune BERT to match questions with answers used in the original Magic Eight Ball toy. My training dataset will include 1,000 questions generated with the help of ChatGPT. I will personally assign the questions with matching categories. The original Magic Eight Balls had four possible answers in each ball, taken from a list of twenty company answers. I will try two versions of the training data for fine-tuning, one with four possible categories and one with all twenty for comparison. 

I will do this in a Jupyter Notebook with markdown annotation and upload it onto Github for public accessibility. For the class pin-up, I will also include a version of the project on a preloaded Colab notebook so my classmates can try entering questions to see the matched responses. 

I want my fine-tuned LLMS to assign answers more consistently than the original toy. However, my primary intent is to highlight the randomness of the mechanics of LLMs by demonstrating the similarities between the two technologies. Like the toy, my fine-tuned model will rely on probabilities to match answers to questions without understanding the future. This project will explore the ways humans put their trust in machine learning models. When the Magic Eight Balls were first popularized, people understood them as toys - not trustworthy soothsayers. However, with increased access to LLMs, there is the risk that LLM hallucinations and inaccuracies will be interpreted as facts.


### The Training Data

### The Build

### The Purpose

### Social Implications
