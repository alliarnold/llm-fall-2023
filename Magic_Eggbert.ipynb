{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Dgz8Fk2RiGsCfapotWXG8gS6vVumjgcH",
      "authorship_tag": "ABX9TyO37YJWZACbYv5w5YL1E0fp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alliarnold/llm-fall-2023/blob/main/Magic_Eggbert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Magic Eight-BERT\n",
        "\n",
        "### Step right up, step right out! Ask the magic LLM  to see into your future!\n",
        "\n",
        "When they first came out, Magic 8 balls were the shinny new toy for jokingly looking into your future through random chance - or dare we say, probability!\n",
        "\n",
        "Today we look to AI to help us predict the future. LLMs, like their Magic 8 ball predecessars, are deeply steeped in the use of measured probabilities and have as much human *understanding* as the original plastic orbe. Unfortunately the average person didn't get to take our class, so they don't realize how similar the toy and the tool are!\n",
        "\n",
        "So take your turn now, and ask the Magic 8-Bert to tell you about your future!\n"
      ],
      "metadata": {
        "id": "Y5nUuIjgDjNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "pGqm673BFLFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "#tqdm is a progress bar\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "MQFvyLFMFRXV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "N_QK6b0BFl7W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5933c119-c4b7-4c01-d24b-b0f8676aeb16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')"
      ],
      "metadata": {
        "id": "TYTEOBE4JcIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes4 =[\"Cannot predict now\",\"Outlook good\",\"Signs point to yes\",\"Outlook not good\"]"
      ],
      "metadata": {
        "id": "_49k15f-IcPl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes20 =[\"It is certain\",\"It is decidedly so\",\"Without a doubt\",\"Yes definitely\",\n",
        "          \"You may rely on it\", \"As I see it, yes\", \"Most likely\", \"Outlook good\",\n",
        "           \"Yes\",\"Signs point to yes\",\"Reply hazy, try again\", \"Ask again later\",\n",
        "           \"Better not tell you now\", \"Cannot predict now\", \"Concentrate and ask again\",\n",
        "           \"Don't count on it\", \"My reply is no\", \"My sources say no\", \"Outlook not so good\",\n",
        "           \"Very doubtful\"]"
      ],
      "metadata": {
        "id": "26iGORHYIm1U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Answer Model"
      ],
      "metadata": {
        "id": "2i_fqgV4EzKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model('/content/drive/MyDrive/GC-CUNY/2023_Fall/magic4model')"
      ],
      "metadata": {
        "id": "oE6g2IycEyIa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Put your question in the cell below."
      ],
      "metadata": {
        "id": "0doOmtMbINem"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_question = \"Should I donate more money to charity?\""
      ],
      "metadata": {
        "id": "ELGK1HORH-II"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## don't look behind the curtain here\n",
        "\n",
        "def prepare_newdata(new_question, tokenizer):\n",
        "  token = tokenizer.encode_plus(\n",
        "      new_question,\n",
        "      max_length=256,\n",
        "      truncation=True,\n",
        "      padding=\"max_length\",\n",
        "      add_special_tokens=True,\n",
        "      return_tensors='tf'\n",
        "\n",
        "  )\n",
        "  return {\n",
        "      'input_ids': tf.cast(token.input_ids, tf.float64),\n",
        "      'attention_mask':tf.cast(token.attention_mask, tf.float64)\n",
        "  }"
      ],
      "metadata": {
        "id": "GI4sxdPyIYuW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_quest = prepare_newdata(new_question, tokenizer)\n",
        "probs = loaded_model.predict(tokenized_quest)"
      ],
      "metadata": {
        "id": "IkdG57RqI4_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Magic4_Answer = np.argmax(probs[0])\n",
        "print(classes4[Magic4_Answer])"
      ],
      "metadata": {
        "id": "sR7HZarCJhjV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbfe2482-6c87-435e-8e90-be15ea19efb9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Outlook good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20 Answer Model"
      ],
      "metadata": {
        "id": "qDnqYqY1J3nc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model20 = tf.keras.models.load_model('/content/drive/MyDrive/GC-CUNY/2023_Fall/magic20model')"
      ],
      "metadata": {
        "id": "9_jgRkFMKlTi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Put your next question in the cell below."
      ],
      "metadata": {
        "id": "lgKoZFXmK8GK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_2 = \"Should I rob a bank?\""
      ],
      "metadata": {
        "id": "I1kVF17rKuVx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## don't look behind the curtain here\n",
        "\n",
        "def prepare_newdata(question_2, tokenizer):\n",
        "  token = tokenizer.encode_plus(\n",
        "      question_2,\n",
        "      max_length=256,\n",
        "      truncation=True,\n",
        "      padding=\"max_length\",\n",
        "      add_special_tokens=True,\n",
        "      return_tensors='tf'\n",
        "\n",
        "  )\n",
        "  return {\n",
        "      'input_ids': tf.cast(token.input_ids, tf.float64),\n",
        "      'attention_mask':tf.cast(token.attention_mask, tf.float64)\n",
        "  }"
      ],
      "metadata": {
        "id": "5bmTu6ulP53j"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_quest2 = prepare_newdata(question_2, tokenizer)\n",
        "probs2 = model20.predict(tokenized_quest2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao2vN4CsP1Ky",
        "outputId": "e50de461-3aba-4732-fcc8-a20205e6e576"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "magic20_answer = np.argmax(probs2[0])\n",
        "print(classes20[magic20_answer])"
      ],
      "metadata": {
        "id": "dqW30_G-PWzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79f1860c-3742-4422-f0f1-55fcee316dfb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My reply is no\n"
          ]
        }
      ]
    }
  ]
}